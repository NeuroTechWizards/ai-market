"""–ó–∞–≥—Ä—É–∑–∫–∞ RFSD –¥–∞–Ω–Ω—ã—Ö (Parquet) –ø–æ –≥–æ–¥–∞–º –∏–∑ Hugging Face."""

from __future__ import annotations

from typing import Iterable, Sequence, Any
import logging

import polars as pl

logger = logging.getLogger(__name__)

_AVAILABLE_YEARS = list(range(2011, 2025))

# –ì–ª–æ–±–∞–ª—å–Ω—ã–π –∫—ç—à –¥–∞–Ω–Ω—ã—Ö –ø–æ –≥–æ–¥–∞–º
_data_cache: dict[int, pl.DataFrame] = {}


def list_available_years() -> list[int]:
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –≥–æ–¥–æ–≤."""
    return _AVAILABLE_YEARS.copy()


def _validate_year(year: int) -> None:
    if year not in _AVAILABLE_YEARS:
        raise ValueError(f"–ì–æ–¥ {year} –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω. –î–æ–ø—É—Å—Ç–∏–º–æ: {min(_AVAILABLE_YEARS)}‚Äì{max(_AVAILABLE_YEARS)}")


from .settings import settings


def _scan_year(year: int, columns: Sequence[str] | None = None) -> pl.LazyFrame:
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ª–µ–Ω–∏–≤—ã–π —Å–∫–∞–Ω –ø–æ –≥–æ–¥—É —Å –¥–æ–±–∞–≤–ª–µ–Ω–Ω–æ–π –∫–æ–ª–æ–Ω–∫–æ–π year.
    
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –∫—ç—à –µ—Å–ª–∏ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –≥–æ–¥–∞ –∑–∞–≥—Ä—É–∂–µ–Ω—ã –≤ –ø–∞–º—è—Ç—å.
    """
    _validate_year(year)
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—ç—à
    if year in _data_cache:
        df = _data_cache[year]
        # –ï—Å–ª–∏ –∑–∞–ø—Ä–æ—à–µ–Ω—ã –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏, —Ñ–∏–ª—å—Ç—Ä—É–µ–º
        if columns is not None:
            available_cols = [col for col in columns if col in df.columns]
            if "year" not in available_cols and "year" in df.columns:
                available_cols.append("year")
            df = df.select(available_cols)
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –∫–∞–∫ LazyFrame –∏–∑ –∫—ç—à–∞
        return df.lazy()
    
    # –ï—Å–ª–∏ –Ω–µ—Ç –≤ –∫—ç—à–µ, —á–∏—Ç–∞–µ–º —Å HF
    path = f"hf://datasets/irlspbru/RFSD/RFSD/year={year}/*.parquet"
    
    storage_options = None
    if settings.HF_TOKEN:
        storage_options = {"token": settings.HF_TOKEN}
        
    scan = pl.scan_parquet(path, storage_options=storage_options)
    scan = scan.with_columns(pl.lit(year).alias("year"))
    if columns is not None:
        scan = scan.select(list(columns))
    return scan


def preload_cache(years: list[int] | None = None) -> None:
    """–ü—Ä–µ–¥–∑–∞–≥—Ä—É–∂–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ —É–∫–∞–∑–∞–Ω–Ω—ã—Ö –≥–æ–¥–æ–≤ –≤ –∫—ç—à.
    
    Args:
        years: –°–ø–∏—Å–æ–∫ –≥–æ–¥–æ–≤ –¥–ª—è –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è. –ï—Å–ª–∏ None, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç CACHE_YEARS –∏–∑ –Ω–∞—Å—Ç—Ä–æ–µ–∫.
    """
    if years is None:
        # –ü–∞—Ä—Å–∏–º CACHE_YEARS –∏–∑ –Ω–∞—Å—Ç—Ä–æ–µ–∫
        cache_years_str = settings.CACHE_YEARS
        try:
            years = [int(y.strip()) for y in cache_years_str.split(",")]
        except ValueError:
            logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å CACHE_YEARS: {cache_years_str}")
            return
    
    logger.info(f"üîÑ –ù–∞—á–∏–Ω–∞—é –ø—Ä–µ–¥–∑–∞–≥—Ä—É–∑–∫—É –∫—ç—à–∞ –¥–ª—è –≥–æ–¥–æ–≤: {years}")
    
    for year in years:
        if year not in _AVAILABLE_YEARS:
            logger.warning(f"‚ö†Ô∏è –ì–æ–¥ {year} –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, –ø—Ä–æ–ø—É—Å–∫–∞—é")
            continue
        
        if year in _data_cache:
            logger.info(f"‚úÖ –ì–æ–¥ {year} —É–∂–µ –≤ –∫—ç—à–µ, –ø—Ä–æ–ø—É—Å–∫–∞—é")
            continue
        
        try:
            logger.info(f"üì• –ó–∞–≥—Ä—É–∂–∞—é –≥–æ–¥ {year} –≤ –∫—ç—à...")
            df = load_year(year)
            _data_cache[year] = df
            logger.info(f"‚úÖ –ì–æ–¥ {year} –∑–∞–≥—Ä—É–∂–µ–Ω –≤ –∫—ç—à ({len(df):,} —Å—Ç—Ä–æ–∫, {df.estimated_size('mb'):.1f} MB)")
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –≥–æ–¥–∞ {year}: {e}")
    
    total_size_mb = sum(df.estimated_size('mb') for df in _data_cache.values())
    logger.info(f"üéâ –ö—ç—à –≥–æ—Ç–æ–≤! –í—Å–µ–≥–æ –≥–æ–¥–æ–≤: {len(_data_cache)}, —Ä–∞–∑–º–µ—Ä: {total_size_mb:.1f} MB")


def clear_cache() -> None:
    """–û—á–∏—â–∞–µ—Ç –∫—ç—à –¥–∞–Ω–Ω—ã—Ö."""
    _data_cache.clear()
    logger.info("üóëÔ∏è –ö—ç—à –æ—á–∏—â–µ–Ω")


def get_cache_info() -> dict[str, Any]:
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∫—ç—à–µ."""
    return {
        "cached_years": sorted(_data_cache.keys()),
        "total_years": len(_data_cache),
        "total_size_mb": round(sum(df.estimated_size('mb') for df in _data_cache.values()), 2),
        "total_rows": sum(len(df) for df in _data_cache.values())
    }


def load_year(year: int, columns: Sequence[str] | None = None) -> pl.DataFrame:
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –æ–¥–∏–Ω –≥–æ–¥ RFSD –∏–∑ Hugging Face Parquet."""
    return _scan_year(year, columns=columns).collect()


def filter_inn_year(
    year: int,
    inn: str,
    columns: Sequence[str],
    limit: int = 200,
) -> pl.DataFrame:
    """–§–∏–ª—å—Ç—Ä –ø–æ –ò–ù–ù –¥–ª—è —É–∫–∞–∑–∞–Ω–Ω–æ–≥–æ –≥–æ–¥–∞."""
    cols = list(columns)
    if "inn" not in cols:
        cols.append("inn")
    return (
        _scan_year(year, columns=cols)
        .filter(pl.col("inn") == inn)
        .limit(limit)
        .collect()
    )


def get_schema_columns(year: int) -> list[str]:
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –∫–æ–ª–æ–Ω–æ–∫ –¥–ª—è —É–∫–∞–∑–∞–Ω–Ω–æ–≥–æ –≥–æ–¥–∞."""
    _validate_year(year)
    path = f"hf://datasets/irlspbru/RFSD/RFSD/year={year}/*.parquet"
    
    storage_options = None
    if settings.HF_TOKEN:
        storage_options = {"token": settings.HF_TOKEN}
        
    scan = pl.scan_parquet(path, storage_options=storage_options)
    schema = scan.collect_schema()
    columns = list(schema.keys())
    if "year" not in columns:
        columns.append("year")
    return columns


def sample_year(year: int, columns: Sequence[str] | None = None, n: int = 5) -> pl.DataFrame:
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø–µ—Ä–≤—ã–µ n —Å—Ç—Ä–æ–∫ —É–∫–∞–∑–∞–Ω–Ω–æ–≥–æ –≥–æ–¥–∞ –±–µ–∑ –ø–æ–ª–Ω–æ–≥–æ collect."""
    return _scan_year(year, columns=columns).limit(n).collect()


def load_indicators_dict() -> dict[str, str]:
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —Å–ø—Ä–∞–≤–æ—á–Ω–∏–∫ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤ –∏–∑ Excel."""
    import os
    import openpyxl
    
    current_dir = os.path.dirname(os.path.abspath(__file__))
    project_root = os.path.dirname(os.path.dirname(current_dir))
    
    possible_paths = [
        os.path.join(project_root, "docs", "databook", "rfsd_databook_ru.xlsx"),
        os.path.join(project_root, "..", "docs", "databook", "rfsd_databook_ru.xlsx"),
        r"D:\OneDrive\–†–∞–±–æ—Ç–∞\AI consalting\–ò–ò-–∞–≥–µ–Ω—Ç –∏–Ω–≤–µ—Å—Ç –¥–∏—Ä\–ü—Ä–æ–≥—Ä–∞–º–º–∞ –∏ –ö–æ–¥\AI market\docs\databook\rfsd_databook_ru.xlsx"
    ]
    
    databook_path = None
    for p in possible_paths:
        if os.path.exists(p):
            databook_path = p
            break
            
    if not databook_path:
        return {}

    try:
        wb = openpyxl.load_workbook(databook_path, read_only=True, data_only=True)
        if 'databook' in wb.sheetnames:
            ws = wb['databook']
        else:
            ws = wb.active
            
        headers = [cell.value for cell in ws[1]]
        try:
            code_idx = headers.index('code')
            name_idx = headers.index('name_ru')
        except ValueError:
            return {}
            
        result = {}
        for row in ws.iter_rows(min_row=2, values_only=True):
            code = row[code_idx]
            name = row[name_idx]
            if code:
                result[str(code)] = str(name) if name else ""
                
        return result
    except Exception as e:
        print(f"Error loading databook: {e}")
        return {}
