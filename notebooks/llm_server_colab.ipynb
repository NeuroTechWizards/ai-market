{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Server –¥–ª—è RFSD Agent (Colab)\n",
    "\n",
    "–≠—Ç–æ—Ç notebook –∑–∞–ø—É—Å–∫–∞–µ—Ç:\n",
    "1. **Mistral-7B-Instruct** –º–æ–¥–µ–ª—å (–±–µ—Å–ø–ª–∞—Ç–Ω–æ)\n",
    "2. FastAPI —Å–µ—Ä–≤–µ—Ä –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∑–∞–ø—Ä–æ—Å–æ–≤\n",
    "3. ngrok —Ç—É–Ω–Ω–µ–ª—å –¥–ª—è –¥–æ—Å—Ç—É–ø–∞ –∏–∑–≤–Ω–µ\n",
    "\n",
    "**‚ö†Ô∏è –í–ê–ñ–ù–û: –ù–∞—Å—Ç—Ä–æ–π—Ç–µ —Ç–æ–∫–µ–Ω ngrok –ü–ï–†–ï–î –∑–∞–ø—É—Å–∫–æ–º:**\n",
    "\n",
    "### –®–∞–≥ 1: –ü–æ–ª—É—á–∏—Ç–µ —Ç–æ–∫–µ–Ω ngrok\n",
    "1. –û—Ç–∫—Ä–æ–π—Ç–µ [ngrok.com/signup](https://dashboard.ngrok.com/signup) (–±–µ—Å–ø–ª–∞—Ç–Ω–æ, 1 –º–∏–Ω—É—Ç–∞)\n",
    "2. –ó–∞—Ä–µ–≥–∏—Å—Ç—Ä–∏—Ä—É–π—Ç–µ—Å—å —á–µ—Ä–µ–∑ Google/GitHub –∏–ª–∏ email\n",
    "3. –°–∫–æ–ø–∏—Ä—É–π—Ç–µ —Ç–æ–∫–µ–Ω —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã [Your Authtoken](https://dashboard.ngrok.com/get-started/your-authtoken)\n",
    "\n",
    "### –®–∞–≥ 2: –î–æ–±–∞–≤—å—Ç–µ —Ç–æ–∫–µ–Ω –≤ Colab Secrets\n",
    "1. –°–ª–µ–≤–∞ –Ω–∞–∂–º–∏—Ç–µ –Ω–∞ –∏–∫–æ–Ω–∫—É **üîë Secrets**\n",
    "2. –ù–∞–∂–º–∏—Ç–µ **+ Add new secret**\n",
    "3. **Name**: `NGROK_TOKEN`\n",
    "4. **Value**: –≤—Å—Ç–∞–≤—å—Ç–µ –≤–∞—à —Ç–æ–∫–µ–Ω\n",
    "5. –í–∫–ª—é—á–∏—Ç–µ –≥–∞–ª–æ—á–∫—É **Notebook access**\n",
    "\n",
    "### –®–∞–≥ 3: –ó–∞–ø—É—Å—Ç–∏—Ç–µ notebook\n",
    "1. –í–∫–ª—é—á–∏—Ç–µ GPU: `Runtime` ‚Üí `Change runtime type` ‚Üí `T4 GPU`\n",
    "2. –ó–∞–ø—É—Å—Ç–∏—Ç–µ –≤—Å–µ —è—á–µ–π–∫–∏: `Runtime` ‚Üí `Run all`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –®–∞–≥ 1: –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –±–∏–±–ª–∏–æ—Ç–µ–∫\n",
    "!pip install -q transformers accelerate bitsandbytes fastapi uvicorn pyngrok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –®–∞–≥ 2: –ù–∞—Å—Ç—Ä–æ–π–∫–∞ ngrok —Ç–æ–∫–µ–Ω–∞ (—á–∏—Ç–∞–µ–º –∏–∑ Colab Secrets)\n",
    "from google.colab import userdata\n",
    "from pyngrok import ngrok\n",
    "\n",
    "try:\n",
    "    NGROK_TOKEN = userdata.get('NGROK_TOKEN')\n",
    "    ngrok.set_auth_token(NGROK_TOKEN)\n",
    "    print(\"‚úÖ –¢–æ–∫–µ–Ω ngrok –Ω–∞—Å—Ç—Ä–æ–µ–Ω (–∏–∑ Secrets)\")\n",
    "except Exception as e:\n",
    "    print(\"‚ùå –û–®–ò–ë–ö–ê: –¢–æ–∫–µ–Ω ngrok –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ Secrets!\")\n",
    "    print(\"\\nüìù –ö–∞–∫ –¥–æ–±–∞–≤–∏—Ç—å —Ç–æ–∫–µ–Ω –≤ Secrets:\")\n",
    "    print(\"1. –°–ª–µ–≤–∞ –Ω–∞–∂–º–∏—Ç–µ –Ω–∞ –∏–∫–æ–Ω–∫—É üîë (Secrets)\")\n",
    "    print(\"2. –ù–∞–∂–º–∏—Ç–µ '+ Add new secret'\")\n",
    "    print(\"3. Name: NGROK_TOKEN\")\n",
    "    print(\"4. Value: –≤–∞—à —Ç–æ–∫–µ–Ω (–ø–æ–ª—É—á–∏—Ç–µ –Ω–∞ https://dashboard.ngrok.com/get-started/your-authtoken)\")\n",
    "    print(\"5. –í–∫–ª—é—á–∏—Ç–µ –≥–∞–ª–æ—á–∫—É 'Notebook access'\")\n",
    "    print(\"6. –ó–∞–ø—É—Å—Ç–∏—Ç–µ —ç—Ç—É —è—á–µ–π–∫—É —Å–Ω–æ–≤–∞\")\n",
    "    raise ValueError(f\"–¢–æ–∫–µ–Ω ngrok –Ω–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –®–∞–≥ 3: –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ (Mistral-7B-Instruct)\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "model_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "# –ò—Å–ø–æ–ª—å–∑—É–µ–º 8-bit –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—é –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏\n",
    "quantization_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "\n",
    "print(\"üîÑ –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ Mistral-7B...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=quantization_config,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "print(\"‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –®–∞–≥ 4: FastAPI —Å–µ—Ä–≤–µ—Ä\n",
    "from fastapi import FastAPI\n",
    "from pydantic import BaseModel\n",
    "import uvicorn\n",
    "from threading import Thread\n",
    "\n",
    "app = FastAPI(title=\"LLM Server\")\n",
    "\n",
    "class ChatRequest(BaseModel):\n",
    "    messages: list[dict]\n",
    "    temperature: float = 0.3\n",
    "    max_tokens: int = 500\n",
    "\n",
    "class ChatResponse(BaseModel):\n",
    "    choices: list[dict]\n",
    "\n",
    "@app.post(\"/v1/chat/completions\")\n",
    "async def chat_completions(request: ChatRequest) -> ChatResponse:\n",
    "    \"\"\"OpenAI-compatible endpoint.\"\"\"\n",
    "    # –§–æ—Ä–º–∏—Ä—É–µ–º –ø—Ä–æ–º–ø—Ç –∏–∑ messages\n",
    "    prompt_parts = []\n",
    "    for msg in request.messages:\n",
    "        role = msg['role']\n",
    "        content = msg['content']\n",
    "        if role == 'system':\n",
    "            prompt_parts.append(f\"System: {content}\")\n",
    "        elif role == 'user':\n",
    "            prompt_parts.append(f\"User: {content}\")\n",
    "    prompt_parts.append(\"Assistant:\")\n",
    "    prompt = \"\\n\".join(prompt_parts)\n",
    "    \n",
    "    # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=request.max_tokens,\n",
    "        temperature=request.temperature,\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    \n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–æ–ª—å–∫–æ –æ—Ç–≤–µ—Ç –º–æ–¥–µ–ª–∏\n",
    "    answer = generated_text.split(\"Assistant:\")[-1].strip()\n",
    "    \n",
    "    return ChatResponse(\n",
    "        choices=[\n",
    "            {\n",
    "                \"message\": {\n",
    "                    \"role\": \"assistant\",\n",
    "                    \"content\": answer\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "\n",
    "@app.get(\"/health\")\n",
    "async def health():\n",
    "    return {\"status\": \"ok\", \"model\": model_id}\n",
    "\n",
    "# –ó–∞–ø—É—Å–∫ –≤ —Ñ–æ–Ω–æ–≤–æ–º –ø–æ—Ç–æ–∫–µ\n",
    "def run_server():\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8000, log_level=\"info\")\n",
    "\n",
    "server_thread = Thread(target=run_server, daemon=True)\n",
    "server_thread.start()\n",
    "\n",
    "print(\"‚úÖ FastAPI –∑–∞–ø—É—â–µ–Ω –Ω–∞ –ø–æ—Ä—Ç—É 8000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –®–∞–≥ 5: ngrok —Ç—É–Ω–Ω–µ–ª—å\n",
    "from pyngrok import ngrok\n",
    "import time\n",
    "\n",
    "# –î–∞—ë–º —Å–µ—Ä–≤–µ—Ä—É –≤—Ä–µ–º—è –∑–∞–ø—É—Å—Ç–∏—Ç—å—Å—è\n",
    "time.sleep(3)\n",
    "\n",
    "# –°–æ–∑–¥–∞—ë–º —Ç—É–Ω–Ω–µ–ª—å\n",
    "tunnel = ngrok.connect(8000)\n",
    "public_url = tunnel.public_url  # –ò–∑–≤–ª–µ–∫–∞–µ–º URL –∫–∞–∫ —Å—Ç—Ä–æ–∫—É\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"‚úÖ LLM Server –∑–∞–ø—É—â–µ–Ω!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"üì° –ü—É–±–ª–∏—á–Ω—ã–π URL: {public_url}\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nüìã –°–ö–û–ü–ò–†–£–ô–¢–ï –≠–¢–û–¢ URL –∏ –¥–æ–±–∞–≤—å—Ç–µ –≤ .env –≤–∞—à–µ–≥–æ –±—ç–∫–µ–Ω–¥–∞:\")\n",
    "print(f\"\\nLLM_ENDPOINT_URL={public_url}\")\n",
    "print(\"\\n–ó–∞—Ç–µ–º –ø–µ—Ä–µ–∑–∞–ø—É—Å—Ç–∏—Ç–µ –±—ç–∫–µ–Ω–¥ (Ctrl+C –∏ uvicorn app.main:app --reload)\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –®–∞–≥ 6: –¢–µ—Å—Ç–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å (–ø—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–±–æ—Ç–æ—Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏)\n",
    "import requests\n",
    "\n",
    "test_payload = {\n",
    "    \"messages\": [\n",
    "        {\"role\": \"system\", \"content\": \"–¢—ã —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–π –∞–Ω–∞–ª–∏—Ç–∏–∫.\"},\n",
    "        {\"role\": \"user\", \"content\": \"–û–±—ä—è—Å–Ω–∏ –∫—Ä–∞—Ç–∫–æ: —á—Ç–æ —Ç–∞–∫–æ–µ –≤—ã—Ä—É—á–∫–∞?\"}\n",
    "    ],\n",
    "    \"temperature\": 0.3,\n",
    "    \"max_tokens\": 100\n",
    "}\n",
    "\n",
    "print(\"üß™ –¢–µ—Å—Ç–∏—Ä—É—é –º–æ–¥–µ–ª—å...\\n\")\n",
    "response = requests.post(f\"{public_url}/v1/chat/completions\", json=test_payload, timeout=60)\n",
    "print(f\"‚úÖ –°—Ç–∞—Ç—É—Å: {response.status_code}\")\n",
    "print(f\"\\nüí¨ –û—Ç–≤–µ—Ç –º–æ–¥–µ–ª–∏:\\n{response.json()['choices'][0]['message']['content']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚úÖ –ì–æ—Ç–æ–≤–æ!\n",
    "\n",
    "### –°–ª–µ–¥—É—é—â–∏–µ —à–∞–≥–∏:\n",
    "\n",
    "1. **–°–∫–æ–ø–∏—Ä—É–π—Ç–µ URL** –∏–∑ —è—á–µ–π–∫–∏ –≤—ã—à–µ (–Ω–∞–ø—Ä–∏–º–µ—Ä: `https://abc123.ngrok-free.app`)\n",
    "\n",
    "2. **–î–æ–±–∞–≤—å—Ç–µ –≤ –±—ç–∫–µ–Ω–¥** (`services/rfsd_backend/.env`):\n",
    "   ```env\n",
    "   LLM_ENDPOINT_URL=https://abc123.ngrok-free.app\n",
    "   ```\n",
    "\n",
    "3. **–ü–µ—Ä–µ–∑–∞–ø—É—Å—Ç–∏—Ç–µ –±—ç–∫–µ–Ω–¥**:\n",
    "   - –í —Ç–µ—Ä–º–∏–Ω–∞–ª–µ: `Ctrl+C`\n",
    "   - –ó–∞—Ç–µ–º: `uvicorn app.main:app --reload --port 8000`\n",
    "\n",
    "4. **–¢–µ—Å—Ç –≤ Telegram –±–æ—Ç–µ**:\n",
    "   ```\n",
    "   –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —Ñ–∏–Ω–∞–Ω—Å–æ–≤–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ –ò–ù–ù 7722514880 –∑–∞ 2023 –∏ –¥–∞–π –≤—ã–≤–æ–¥—ã\n",
    "   ```\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ö†Ô∏è –í–∞–∂–Ω—ã–µ –∑–∞–º–µ—Ç–∫–∏:\n",
    "\n",
    "- **–í—Ä–µ–º—è –∂–∏–∑–Ω–∏:** Colab —Å–µ—Å—Å–∏—è –∂–∏–≤—ë—Ç ~12 —á–∞—Å–æ–≤ (–±–µ–∑ –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏). –ï—Å–ª–∏ –æ—Ç–∫–ª—é—á–∏–ª–∞—Å—å ‚Äî –ø–µ—Ä–µ–∑–∞–ø—É—Å—Ç–∏—Ç–µ `Runtime` ‚Üí `Run all`.\n",
    "- **URL –∏–∑–º–µ–Ω–∏—Ç—Å—è** –ø—Ä–∏ –∫–∞–∂–¥–æ–º –ø–µ—Ä–µ–∑–∞–ø—É—Å–∫–µ ‚Äî –æ–±–Ω–æ–≤–ª—è–π—Ç–µ `.env`.\n",
    "- **–ü–µ—Ä–≤—ã–π –∑–∞–ø—Ä–æ—Å –º–µ–¥–ª–µ–Ω–Ω—ã–π** (~20-30 —Å–µ–∫), –∑–∞—Ç–µ–º –±—ã—Å—Ç—Ä–µ–µ (~5-10 —Å–µ–∫).\n",
    "- **ngrok –±–µ—Å–ø–ª–∞—Ç–Ω–æ:** 1 –∞–∫—Ç–∏–≤–Ω—ã–π —Ç—É–Ω–Ω–µ–ª—å, –Ω–µ–æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–æ–µ –≤—Ä–µ–º—è."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
